<html>

<head>
<title>Archive Engine</title>
</head>

<body BGCOLOR="#F0F0FF" BACKGROUND="../blueback.jpg">
<font FACE="Comic Sans MS,Arial, Helvetica">

<blockquote>
  <h1>Hardware, Configuration</h1>
  <p>All I/O for the binary archive format is based on the LowLevelIO class in the Tools
  directory.<br>
  Via ToolsConfig.h, this class can be configured to use &quot;fd&quot; file descriptor I/O
  (open, write, lseek, ...) or &quot;FILE *&quot; calls from stdio (fopen, fwrite, ...).<br>
  In addition, all ChannelArchiver files can be compiled either with debug information or
  optimized, depending on the HOST_OPT setting which is usually set in
  EPICS/base/config/CONFIG_SITE but can be overwritten in ArchiverConfig.h. </p>
  <p>The ChannelArchiver/Engine directory contains a simple benchmark program bench.cpp.
  Depending on the I/O and debug settings, extremely different results were obtained: </p>
  <table BORDER="2" CELLPADDING="5">
    <tr>
      <th>Machine </th>
      <th>Settings</th>
      <th>Values/sec. written</th>
    </tr>
    <tr>
      <td>800Mhz NT4.0 </td>
      <td align="center">fd </td>
      <td align="right">35000</td>
    </tr>
    <tr>
      <td>800Mhz NT4.0 </td>
      <td align="center">FILE </td>
      <td align="right">20000</td>
    </tr>
    <tr>
      <td>500Mhz RH 6.1 </td>
      <td align="center">debug </td>
      <td align="right">60 (!)</td>
    </tr>
    <tr>
      <td>500Mhz RH 6.1 </td>
      <td align="center">FILE, -O</td>
      <td align="right">14500</td>
    </tr>
    <tr>
      <td>2x333Mhz, RAID5</td>
      <td align="center">debug </td>
      <td align="right">42 (!)</td>
    </tr>
    <tr>
      <td>2x333Mhz, RAID5</td>
      <td align="center">FILE, -O</td>
      <td align="right">8600</td>
    </tr>
  </table>
  <p>In the Win32 case, raw fd I/O seems to be faster, debug settings have little influence.<br>
  For Linux, fd I/O is terribly slow in any case. So is FILE I/O without using the
  optimizing -O flag. <i>For Linux, only FILE I/O with optimization yields acceptable
  results.</i></p>
  <p>This underlying I/O performace limits the number of values that the ArchiveEngine can
  handle.</p>
  <h1>Archive Engine Limits</h1>
  <h2>Test: &quot;Up to 10000 values per second&quot;</h2>
  <p>The ArchiveEngine was started with this configuration file:</p>
  <pre>  #Archive channels of example CA server (excas)
  !file_size 10
  !write_period 10
  fred 1.0 Monitor
  freddy 1.0 Monitor
  janet 0.1 Monitor
  alan 1.0 Monitor
  jane0 0.1 Monitor
  jane1 0.1 Monitor
  jane2 0.1 Monitor
  # ... and so on until
  jane999 0.1 Monitor
</pre>
  <p>Those Channels were served by the example CA server, running on a 233Mhz Linux machine,
  launched as:</p>
  <pre>excas -c1000</pre>
  <p>The &quot;jane&quot; channels change at about 10Hz. Together with the other channels,
  including the array &quot;alan&quot;, this should provide at least 10000 values per
  second.</p>
  <p>Both machines were on a 10/100 base T hub, but the Linux box only supports 10baseT.</p>
  <h2>Observed behaviour</h2>
  <p>This plot shows the CPU load on a 800MHz PC (Windows NT 4.0) archiving about 10000
  values per second:</p>
  <p><img src="cpu_10kpersec.gif" width="343" height="116"
  alt="cpu_10kpersec.gif (5653 bytes)"></p>
  <p>While the machine is quite busy archiving, it does still respond to user input. It
  cannot be used for much else, though: Additional load like launching and using
  PaintShowPro for creating this CPU-load snapshot can cause delays, resulting in messages<br>
  &nbsp;&nbsp;&nbsp; &quot;Warning: WriteThread called while busy&quot;<br>
  More load can lead to &quot;overwrite&quot; errors = data loss and is to be avoided.</p>
  <p>This image shows the limit for the less performant Linux machine where bench.cpp
  reported about 7700 writes/second (233 Mhz, RedHat 6.1):</p>
  <p><img src="ioc94_4kpersec.gif" width="478" height="139"
  alt="ioc94_4kpersec.gif (6210 bytes)"></p>
  <p>The setup is similar to the 10000 val/sec example but uses only 4000 values per second.</p>
  <p>So one could conclude that archiving rates of 10k values/sec are possible on a
  dedicated machine, except for the additional problem of ...</p>
  <h2>Channel Access Flow Control</h2>
  <p>The Archive Engine uses a dedicated write thread to balance the CPU load. It also tries
  to increase the default TCL buffer size in order to buffer incoming values while the
  program is busy writing samples to the disk.</p>
  <p>The current Channel Access library, though, silently implements a flow control
  mechanism. It is not based on a TCP buffer &quot;high-water-mark&quot; idea, instead it
  can drop values when the &quot;ca_poll&quot; routine is called succesfully several times,
  causing it to believe that it might get behind the CA server.</p>
  <p>Ways to detect losses due to flow control are:<ul>
    <li>Instrument the CA server code, e.g. on a test IOC, to display dropped value counts.</li>
    <li>Have an IOC serve 0...10 ramps and compile the engine with CA_STATISTICS defined.<br>
      (an ArchiveEngine compiled this way can only be used for this specific test!)</font><font
      FACE="Comic Sans MS,Arial, Helvetica"></li>
  </ul>
  <p>When this was done the result showed occasional flow control losses at 10000 values per
  second, so the conclusion is that the current engine can archive &quot;close to 10000
  vps&quot;.</p>
  <h2>Behaviour when approaching limit</h2>
  <p>When archiving more and more values per second, the write threads needs an increased
  amount of time. The peaks in the CPU load snapshot show the write thread running. Because
  the write thread takes certain mutex semaphores, the engine's web server is delayed while
  writing. (If the web server request does not need to access the channel data as in the
  case of the &quot;stop&quot; request, the server and write thread actually run
  concurrently, depending only on the scheduling as handled by the operating system.)</p>
  <p>When even more values are received, the write thread cannot flush the buffers within
  the configured write period. If it happens rarely, it will simply result in &quot;write
  thread called while busy&quot; messages and no data loss. If the incoming data rate is too
  high, additional &quot;overwrite&quot; errors will occur, data is lost.</p>
  <p>The web server will always have to wait for the write thread to finish. Consequently,
  occasional delays in a browser request are expected. While approaching the load limit,
  this will happen more often up to the point where the web interface does no longer respond
  because the write thread is constantly running and data is lost.</p>
</blockquote>
</font>
</body>
</html>
